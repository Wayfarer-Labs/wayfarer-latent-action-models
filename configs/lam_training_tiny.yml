ckpt_dir: checkpoints            # where checkpoints go
resume_checkpoint: null          # path to *.pt to resume from (or null)

# -- optimizer
lr: 2.5e-5                       # AdamW learning-rate
weight_decay: 0.01
betas: [0.9, 0.999]
amp: true                        # mixed-precision (16-mixed)
max_grad_norm: 0.3               # gradient-clip value

# -- training loop
max_steps: 1000000
log_every: 1000                  # log interval
ckpt_every: 5000                 # save interval

# -- model
video_dims: [256, 256]           # H, W
in_dim: 3                        # image_channels
model_dim: 1024                  # lam_model_dim
vae_dim: 32                      # lam_latent_dim
patch_size: 16                   # lam_patch_size
num_enc_blocks: 16               # lam_enc_blocks
num_dec_blocks: 16               # lam_dec_blocks
num_heads: 16                    # lam_num_heads
dropout: 0.0
beta: 0.0002                     # KL weight

# -- dataset
data:
  batch_size: 64                 # keep in sync with trainer batch_size
  data_root: ../data
  env_source: game
  padding: repeat
  randomize: true
  resolution: 256
  num_frames: 2
  output_format: "t h w c"
  samples_per_epoch: 10000000
  sampling_strategy: pi
