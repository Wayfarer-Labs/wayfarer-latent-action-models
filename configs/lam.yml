ckpt_dir: checkpoints            # where checkpoints go
resume_checkpoint: null          # path to *.pt to resume from (or null)

# -- optimizer
lr: 3.0e-6                       # AdamW learning-rate
weight_decay: 0.01
betas: [0.9, 0.999]
amp: true                        # mixed-precision (16-mixed)
max_grad_norm: 1.5               # gradient-clip value

# -- training loop
max_steps: 1000000
log_every: 1                  # log interval
ckpt_every: 5000                 # save interval

# -- model
video_dims: [256, 256]           # H, W
in_dim: 3                        # image_channels
model_dim: 1024                  # lam_model_dim
vae_dim: 32                      # lam_latent_dim
patch_size: 16                   # lam_patch_size
num_enc_blocks: 16               # lam_enc_blocks
num_dec_blocks: 16               # lam_dec_blocks
num_heads: 16                    # lam_num_heads
dropout: 0.0
beta: 0.0002                     # KL weight
val_num_samples_umap: 50
val_num_samples_recon: 5
val_every: 250

# -- conditoning
conditioning: 'crossattn'

# -- dataset
data:
  batch_size: 64                 # keep in sync with trainer batch_size
  dataset_name: owl_data
  num_frames: 2
  resolution: 256
  samples_per_epoch: 100000
  num_workers: 0